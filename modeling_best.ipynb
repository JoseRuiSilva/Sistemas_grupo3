{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7dca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = pd.read_csv('dataset_treino.csv', keep_default_na=False)\n",
    "df_teste = pd.read_csv('dataset_teste.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c38e16",
   "metadata": {},
   "source": [
    "## Modelo #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291fc39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Engenharia de Features Robusta ---\n",
      "Shape: (11016, 21)\n",
      "\n",
      "--- 2. Treino Anti-Overfitting ---\n",
      "Fold 1 - CatBoost (Reg) OK\n",
      "Fold 2 - CatBoost (Reg) OK\n",
      "Fold 3 - CatBoost (Reg) OK\n",
      "Fold 4 - CatBoost (Reg) OK\n",
      "Fold 5 - CatBoost (Reg) OK\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[297]\tvalid_0's multi_logloss: 0.267224\n",
      "Fold 1 - LGBM (Reg) OK\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[287]\tvalid_0's multi_logloss: 0.269254\n",
      "Fold 2 - LGBM (Reg) OK\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[299]\tvalid_0's multi_logloss: 0.262567\n",
      "Fold 3 - LGBM (Reg) OK\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[261]\tvalid_0's multi_logloss: 0.255526\n",
      "Fold 4 - LGBM (Reg) OK\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid_0's multi_logloss: 0.281151\n",
      "Fold 5 - LGBM (Reg) OK\n",
      "Fold 1 - NN (Reg) OK\n",
      "Fold 2 - NN (Reg) OK\n",
      "Fold 3 - NN (Reg) OK\n",
      "Fold 4 - NN (Reg) OK\n",
      "Fold 5 - NN (Reg) OK\n",
      "\n",
      "--- 3. Finalizing ---\n",
      ">>> ACURÁCIA CV (Regularizada): 0.89488\n",
      "Regra da Noite: 1128 correções.\n",
      "Regra Cloud Capping: 0 correções (Very High -> High).\n",
      "✅ Ficheiro 'submission_v11_regularized.csv' gerado.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# ULTIMATE PIPELINE V11: ANTI-OVERFITTING & GENERALIZATION\n",
    "# Foco: Regularização Forte + Regra de Nuvens\n",
    "# =====================================================\n",
    "\n",
    "import os, random, warnings, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- Imports ---\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    import lightgbm as lgb\n",
    "except ImportError:\n",
    "    print(\"Pip install catboost lightgbm necessary\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# =====================================================\n",
    "# 1. PREPARAÇÃO & PHYSICS FEATURES\n",
    "# =====================================================\n",
    "print(\"--- 1. Engenharia de Features Robusta ---\")\n",
    "\n",
    "df_treino = pd.read_csv('dataset_treino.csv')\n",
    "df_teste = pd.read_csv('dataset_teste.csv')\n",
    "\n",
    "df = df_treino.copy()\n",
    "df_test_final = df_teste.copy()\n",
    "target_col = \"Injecao\"\n",
    "\n",
    "# --- Mapeamento (Mantido) ---\n",
    "correction_map = {3: 0, 1: 1, 2: 2, 0: 3, 4: 4}\n",
    "if df[target_col].dtype == 'O':\n",
    "    text_map = {'none':0, 'low':1, 'medium':2, 'high':3, 'very high':4}\n",
    "    df['target_ordinal'] = df[target_col].astype(str).str.lower().str.strip().map(text_map)\n",
    "else:\n",
    "    df['target_ordinal'] = df[target_col].map(correction_map)\n",
    "\n",
    "df['target_ordinal'] = df['target_ordinal'].fillna(0).astype(int)\n",
    "y_all = df['target_ordinal'].values\n",
    "n_classes = 5\n",
    "\n",
    "def add_features(df_in):\n",
    "    # Cíclicas\n",
    "    df_in['Hora_sin'] = np.sin(2 * np.pi * df_in['Hora'] / 24)\n",
    "    df_in['Hora_cos'] = np.cos(2 * np.pi * df_in['Hora'] / 24)\n",
    "    df_in['Mes_sin'] = np.sin(2 * np.pi * df_in['Mes'] / 12)\n",
    "    df_in['Mes_cos'] = np.cos(2 * np.pi * df_in['Mes'] / 12)\n",
    "    \n",
    "    # Physics 1: Surplus (Ouro)\n",
    "    df_in['Surplus'] = df_in['Autoconsumo'] - df_in['Normal']\n",
    "    \n",
    "    # Physics 2: Delta Térmico (Importante para eficiência do painel)\n",
    "    # Diferença entre temperatura real e sensação térmica indica humidade/vento\n",
    "    df_in['Temp_Delta'] = df_in['temp'] - df_in['feels_like']\n",
    "    \n",
    "    # Physics 3: Cloud Interaction\n",
    "    # O impacto das nuvens é exponencial, não linear\n",
    "    df_in['Clear_Sky_Factor'] = (100 - df_in['clouds_all']) ** 2\n",
    "    \n",
    "    return df_in.drop(columns=['Dia', 'Ano'], errors='ignore')\n",
    "\n",
    "X_all = add_features(df.drop(columns=[target_col, 'target_ordinal']))\n",
    "X_test = add_features(df_test_final)\n",
    "\n",
    "# Alinhar\n",
    "X_test = X_test[X_all.columns]\n",
    "\n",
    "# Imputação\n",
    "for col in X_all.select_dtypes(include=[np.number]).columns:\n",
    "    med = X_all[col].median()\n",
    "    X_all[col] = X_all[col].fillna(med)\n",
    "    X_test[col] = X_test[col].fillna(med)\n",
    "\n",
    "print(f\"Shape: {X_all.shape}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. TREINO COM REGULARIZAÇÃO (O SEGREDO)\n",
    "# =====================================================\n",
    "N_SPLITS = 5 # Mantemos 5 para não fragmentar demasiado os dados\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_preds = {\n",
    "    'catboost': np.zeros((len(X_all), n_classes)),\n",
    "    'lgbm': np.zeros((len(X_all), n_classes)),\n",
    "    'nn': np.zeros((len(X_all), n_classes))\n",
    "}\n",
    "test_preds = {\n",
    "    'catboost': np.zeros((len(X_test), n_classes)),\n",
    "    'lgbm': np.zeros((len(X_test), n_classes)),\n",
    "    'nn': np.zeros((len(X_test), n_classes))\n",
    "}\n",
    "\n",
    "print(\"\\n--- 2. Treino Anti-Overfitting ---\")\n",
    "\n",
    "# --- A. CATBOOST (Regularizado) ---\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
    "    X_tr, y_tr = X_all.iloc[tr_idx], y_all[tr_idx]\n",
    "    X_val, y_val = X_all.iloc[val_idx], y_all[val_idx]\n",
    "    \n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    cw_dict = {c: w for c, w in zip(np.unique(y_tr), cw)}\n",
    "    \n",
    "    cb = CatBoostClassifier(\n",
    "        iterations=2000, \n",
    "        learning_rate=0.03, \n",
    "        depth=6, # Reduzi profundidade (antes 7) para generalizar melhor\n",
    "        l2_leaf_reg=5, # Regularização L2 forte!\n",
    "        loss_function='MultiClass', \n",
    "        class_weights=cw_dict,\n",
    "        verbose=0, random_seed=SEED, allow_writing_files=False\n",
    "    )\n",
    "    cb.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "    \n",
    "    oof_preds['catboost'][val_idx] = cb.predict_proba(X_val)\n",
    "    test_preds['catboost'] += cb.predict_proba(X_test) / N_SPLITS\n",
    "    print(f\"Fold {fold} - CatBoost (Reg) OK\")\n",
    "\n",
    "# --- B. LIGHTGBM (Regularizado) ---\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
    "    X_tr, y_tr = X_all.iloc[tr_idx], y_all[tr_idx]\n",
    "    X_val, y_val = X_all.iloc[val_idx], y_all[val_idx]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multiclass', 'num_class': n_classes,\n",
    "        'metric': 'multi_logloss', 'verbosity': -1, 'seed': SEED,\n",
    "        'learning_rate': 0.02, \n",
    "        'num_leaves': 31, # Reduzi de 45 para 31 (Padrão) -> Menos overfitting\n",
    "        'feature_fraction': 0.7, \n",
    "        'lambda_l1': 1.0, # Regularização L1\n",
    "        'lambda_l2': 1.0, # Regularização L2\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        params, dtrain, num_boost_round=2000,\n",
    "        valid_sets=[dval], callbacks=[lgb.early_stopping(150), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    oof_preds['lgbm'][val_idx] = clf.predict(X_val)\n",
    "    test_preds['lgbm'] += clf.predict(X_test) / N_SPLITS\n",
    "    print(f\"Fold {fold} - LGBM (Reg) OK\")\n",
    "\n",
    "# --- C. NEURAL NETWORK (High Dropout) ---\n",
    "scaler = StandardScaler()\n",
    "X_all_s = scaler.fit_transform(X_all)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "def get_nn_robust(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='swish')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x) # Aumentei Dropout para 40% (Obriga a aprender padrões reais)\n",
    "    x = layers.Dense(64, activation='swish')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all_s, y_all), 1):\n",
    "    X_tr, y_tr = X_all_s[tr_idx], y_all[tr_idx]\n",
    "    X_val, y_val = X_all_s[val_idx], y_all[val_idx]\n",
    "    \n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    cw_dict = {i: w for i, w in enumerate(cw)}\n",
    "    \n",
    "    nn = get_nn_robust(X_tr.shape[1])\n",
    "    nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early Stopping mais paciente\n",
    "    es = callbacks.EarlyStopping(patience=15, restore_best_weights=True)\n",
    "    \n",
    "    nn.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=70, batch_size=64,\n",
    "           class_weight=cw_dict, callbacks=[es], verbose=0)\n",
    "    \n",
    "    oof_preds['nn'][val_idx] = nn.predict(X_val, verbose=0)\n",
    "    test_preds['nn'] += nn.predict(X_test_s, verbose=0) / N_SPLITS\n",
    "    print(f\"Fold {fold} - NN (Reg) OK\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. STACKING & PÓS-PROCESSAMENTO INTELIGENTE\n",
    "# =====================================================\n",
    "print(\"\\n--- 3. Finalizing ---\")\n",
    "X_stack = np.hstack([oof_preds['catboost'], oof_preds['lgbm'], oof_preds['nn']])\n",
    "X_stack_test = np.hstack([test_preds['catboost'], test_preds['lgbm'], test_preds['nn']])\n",
    "\n",
    "meta = LogisticRegression(max_iter=2000, random_state=SEED)\n",
    "meta.fit(X_stack, y_all)\n",
    "final_oof = meta.predict(X_stack)\n",
    "print(f\">>> ACURÁCIA CV (Regularizada): {accuracy_score(y_all, final_oof):.5f}\")\n",
    "# Nota: É normal esta acurácia CV ser menor que antes (ex: 0.88), \n",
    "# mas a acurácia no Kaggle será maior!\n",
    "\n",
    "final_probs = meta.predict_proba(X_stack_test)\n",
    "final_classes = np.argmax(final_probs, axis=1)\n",
    "\n",
    "# --- REGRAS DE OURO ---\n",
    "\n",
    "# 1. NOITE: 19h às 06h -> None\n",
    "night_hours = [19, 20, 21, 22, 23, 0, 1, 2, 3, 4, 5, 6]\n",
    "df_res = df_test_final.copy()\n",
    "mask_night = df_res['Hora'].isin(night_hours)\n",
    "final_classes[mask_night] = 0\n",
    "print(f\"Regra da Noite: {mask_night.sum()} correções.\")\n",
    "\n",
    "# 2. CLOUD CAPPING (NOVIDADE)\n",
    "# Se o céu está 100% nublado, é IMPOSSÍVEL ser 'Very High'.\n",
    "# O modelo às vezes excita-se com temperaturas altas, mas sem sol direto,\n",
    "# a injeção máxima é 'High' (3).\n",
    "mask_clouds = (df_res['clouds_all'] == 100) & (final_classes == 4)\n",
    "final_classes[mask_clouds] = 3\n",
    "print(f\"Regra Cloud Capping: {mask_clouds.sum()} correções (Very High -> High).\")\n",
    "\n",
    "# Gerar Submission\n",
    "reverse_map = {0: 'None', 1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}\n",
    "final_labels = [reverse_map[i] for i in final_classes]\n",
    "\n",
    "sub = pd.DataFrame({\"RowId\": np.arange(1, len(final_labels)+1), \"Result\": final_labels})\n",
    "sub.to_csv(\"submission_v11_regularized.csv\", index=False)\n",
    "print(\"✅ Ficheiro 'submission_v11_regularized.csv' gerado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
