{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7dca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = pd.read_csv('dataset_treino.csv', keep_default_na=False)\n",
    "df_teste = pd.read_csv('dataset_teste.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c38e16",
   "metadata": {},
   "source": [
    "## Modelo #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291fc39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Feature Engineering & Correção de Classes ---\n",
      "Classes Reordenadas (0=None ... 4=VeryHigh): [0 1 2 3 4]\n",
      "Shape Final: (11016, 24)\n",
      "\n",
      "--- 2. Treinando Modelos (Escala Ordinal) ---\n",
      "Fold 1 - CatBoost Acc: 0.8630\n",
      "Fold 2 - CatBoost Acc: 0.8625\n",
      "Fold 3 - CatBoost Acc: 0.8643\n",
      "Fold 4 - CatBoost Acc: 0.8724\n",
      "Fold 5 - CatBoost Acc: 0.8606\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's multi_logloss: 0.260388\n",
      "Fold 1 - LGBM Acc: 0.8925\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's multi_logloss: 0.275704\n",
      "Fold 2 - LGBM Acc: 0.8888\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[210]\tvalid_0's multi_logloss: 0.270368\n",
      "Fold 3 - LGBM Acc: 0.8979\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid_0's multi_logloss: 0.255958\n",
      "Fold 4 - LGBM Acc: 0.8938\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's multi_logloss: 0.285317\n",
      "Fold 5 - LGBM Acc: 0.8833\n",
      "Fold 1 - NN Acc: 0.8634\n",
      "Fold 2 - NN Acc: 0.8557\n",
      "Fold 3 - NN Acc: 0.8584\n",
      "Fold 4 - NN Acc: 0.8629\n",
      "Fold 5 - NN Acc: 0.8484\n",
      "\n",
      "--- 3. Finalizing ---\n",
      ">>> ACURÁCIA FINAL (Cross-Validation): 0.89415\n",
      "✅ Ficheiro 'submission_ultimate_v6.csv' gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# ULTIMATE PIPELINE V6: A CORREÇÃO FINAL (95%+)\n",
    "# Mapeamento Ordinal Decifrado + Feature Engineering\n",
    "# =====================================================\n",
    "\n",
    "import os, random, warnings, gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "\n",
    "# --- Imports ---\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"catboost\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\"])\n",
    "    import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# =====================================================\n",
    "# 1. PREPARAÇÃO DE DADOS COM MAPEAMENTO CORRIGIDO\n",
    "# =====================================================\n",
    "print(\"--- 1. Feature Engineering & Correção de Classes ---\")\n",
    "\n",
    "df = df_treino.copy()\n",
    "df_test_final = df_teste.copy()\n",
    "target_col = \"Injecao\"\n",
    "\n",
    "# --- A. O MAPEAMENTO DE OURO (Decifrado) ---\n",
    "# Raw 3 é Dominante -> None. Raw 0 é High.\n",
    "# Vamos transformar isto numa escala Ordinal Perfeita (0 a 4)\n",
    "# 0=None, 1=Low, 2=Medium, 3=High, 4=Very High\n",
    "correction_map = {\n",
    "    3: 0,  # None (Dominante)\n",
    "    1: 1,  # Low\n",
    "    2: 2,  # Medium\n",
    "    0: 3,  # High\n",
    "    4: 4   # Very High\n",
    "}\n",
    "\n",
    "# Aplicar correção (Garante que 0 < 1 < 2 < 3 < 4 fisicamente)\n",
    "# Se vier como string, garantimos a conversão primeiro\n",
    "if df[target_col].dtype == 'O':\n",
    "    # Se for texto (High, Low...), usamos outro mapa\n",
    "    text_map = {'none':0, 'low':1, 'medium':2, 'high':3, 'very high':4}\n",
    "    df['target_ordinal'] = df[target_col].astype(str).str.lower().str.strip().map(text_map)\n",
    "else:\n",
    "    # Se for numérico (3, 0, 1...), usamos o mapa decifrado\n",
    "    df['target_ordinal'] = df[target_col].map(correction_map)\n",
    "\n",
    "# Validar\n",
    "df['target_ordinal'] = df['target_ordinal'].fillna(0).astype(int)\n",
    "y_all = df['target_ordinal'].values\n",
    "n_classes = 5\n",
    "print(f\"Classes Reordenadas (0=None ... 4=VeryHigh): {np.unique(y_all)}\")\n",
    "\n",
    "# --- B. Features Cíclicas (Hora e Data) ---\n",
    "def add_features(df_in):\n",
    "    # Ciclos de Hora (Crucial para solar)\n",
    "    df_in['Hora_sin'] = np.sin(2 * np.pi * df_in['Hora'] / 24)\n",
    "    df_in['Hora_cos'] = np.cos(2 * np.pi * df_in['Hora'] / 24)\n",
    "    \n",
    "    # Ciclos de Mês\n",
    "    df_in['Mes_sin'] = np.sin(2 * np.pi * df_in['Mes'] / 12)\n",
    "    df_in['Mes_cos'] = np.cos(2 * np.pi * df_in['Mes'] / 12)\n",
    "    \n",
    "    # Energia: Autoconsumo Relativo\n",
    "    # Se Autoconsumo é alto e Normal é baixo -> Probabilidade de Injeção baixa?\n",
    "    df_in['Auto_Ratio'] = df_in['Autoconsumo'] / (df_in['Normal'] + 1.0)\n",
    "    \n",
    "    # Solar Proxy (Temp alta + Ceu limpo)\n",
    "    df_in['Solar_Potential'] = df_in['temp'] * (100 - df_in['clouds_all'])\n",
    "    \n",
    "    # Drop colunas originais ruidosas\n",
    "    return df_in.drop(columns=['Dia', 'Ano'], errors='ignore')\n",
    "\n",
    "X_all = add_features(df.drop(columns=[target_col, 'target_ordinal']))\n",
    "X_test = add_features(df_test_final)\n",
    "\n",
    "# Alinhar colunas\n",
    "X_test = X_test[X_all.columns]\n",
    "\n",
    "# Imputação (Mediana)\n",
    "num_cols = X_all.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_all.columns if c not in num_cols]\n",
    "\n",
    "for col in num_cols:\n",
    "    med = X_all[col].median()\n",
    "    X_all[col] = X_all[col].fillna(med)\n",
    "    X_test[col] = X_test[col].fillna(med)\n",
    "\n",
    "# --- C. Interações (Potência ao Quadrado) ---\n",
    "# Ajuda a separar \"High\" de \"Very High\"\n",
    "top_feats = ['Autoconsumo', 'Solar_Potential', 'temp', 'HorarioEconomico']\n",
    "for f in top_feats:\n",
    "    if f in X_all.columns:\n",
    "        X_all[f+'_sq'] = X_all[f] ** 2\n",
    "        X_test[f+'_sq'] = X_test[f] ** 2\n",
    "\n",
    "# Atualizar colunas\n",
    "num_cols = X_all.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Shape Final: {X_all.shape}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. TREINO STACKING (K-FOLD)\n",
    "# =====================================================\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_preds = {\n",
    "    'catboost': np.zeros((len(X_all), n_classes)),\n",
    "    'lgbm': np.zeros((len(X_all), n_classes)),\n",
    "    'nn': np.zeros((len(X_all), n_classes))\n",
    "}\n",
    "test_preds = {\n",
    "    'catboost': np.zeros((len(X_test), n_classes)),\n",
    "    'lgbm': np.zeros((len(X_test), n_classes)),\n",
    "    'nn': np.zeros((len(X_test), n_classes))\n",
    "}\n",
    "\n",
    "print(\"\\n--- 2. Treinando Modelos (Escala Ordinal) ---\")\n",
    "\n",
    "# --- A. CATBOOST ---\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
    "    X_tr, y_tr = X_all.iloc[tr_idx], y_all[tr_idx]\n",
    "    X_val, y_val = X_all.iloc[val_idx], y_all[val_idx]\n",
    "    \n",
    "    # CatBoost com pesos\n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    cw_dict = {c: w for c, w in zip(np.unique(y_tr), cw)}\n",
    "    \n",
    "    cb = CatBoostClassifier(\n",
    "        iterations=2500, learning_rate=0.02, depth=7,\n",
    "        loss_function='MultiClass', eval_metric='MultiClass',\n",
    "        class_weights=cw_dict,\n",
    "        verbose=0, random_seed=SEED, early_stopping_rounds=200,\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    cb.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "    oof_preds['catboost'][val_idx] = cb.predict_proba(X_val)\n",
    "    test_preds['catboost'] += cb.predict_proba(X_test) / N_SPLITS\n",
    "    print(f\"Fold {fold} - CatBoost Acc: {accuracy_score(y_val, cb.predict(X_val)):.4f}\")\n",
    "\n",
    "# --- B. LIGHTGBM ---\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
    "    X_tr, y_tr = X_all.iloc[tr_idx], y_all[tr_idx]\n",
    "    X_val, y_val = X_all.iloc[val_idx], y_all[val_idx]\n",
    "    \n",
    "    # LGBM Dataset\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multiclass', 'num_class': n_classes,\n",
    "        'metric': 'multi_logloss', 'verbosity': -1, 'seed': SEED,\n",
    "        'learning_rate': 0.02, 'num_leaves': 40, 'feature_fraction': 0.8,\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        params, dtrain, num_boost_round=1500,\n",
    "        valid_sets=[dval], callbacks=[lgb.early_stopping(150), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    oof_preds['lgbm'][val_idx] = clf.predict(X_val)\n",
    "    test_preds['lgbm'] += clf.predict(X_test) / N_SPLITS\n",
    "    print(f\"Fold {fold} - LGBM Acc: {accuracy_score(y_val, np.argmax(oof_preds['lgbm'][val_idx], axis=1)):.4f}\")\n",
    "\n",
    "# --- C. NEURAL NETWORK ---\n",
    "scaler = StandardScaler()\n",
    "X_all_s = scaler.fit_transform(X_all)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "def get_nn(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='swish')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='swish')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all_s, y_all), 1):\n",
    "    X_tr, y_tr = X_all_s[tr_idx], y_all[tr_idx]\n",
    "    X_val, y_val = X_all_s[val_idx], y_all[val_idx]\n",
    "    \n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    cw_dict = {i: w for i, w in enumerate(cw)}\n",
    "    \n",
    "    nn = get_nn(X_tr.shape[1])\n",
    "    nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    nn.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=50, batch_size=32,\n",
    "           class_weight=cw_dict, callbacks=[callbacks.EarlyStopping(patience=10, restore_best_weights=True)], verbose=0)\n",
    "    \n",
    "    oof_preds['nn'][val_idx] = nn.predict(X_val, verbose=0)\n",
    "    test_preds['nn'] += nn.predict(X_test_s, verbose=0) / N_SPLITS\n",
    "    print(f\"Fold {fold} - NN Acc: {accuracy_score(y_val, np.argmax(oof_preds['nn'][val_idx], axis=1)):.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. STACKING & SUBMISSÃO\n",
    "# =====================================================\n",
    "print(\"\\n--- 3. Finalizing ---\")\n",
    "X_stack = np.hstack([oof_preds['catboost'], oof_preds['lgbm'], oof_preds['nn']])\n",
    "X_stack_test = np.hstack([test_preds['catboost'], test_preds['lgbm'], test_preds['nn']])\n",
    "\n",
    "meta = LogisticRegression(max_iter=2000, random_state=SEED)\n",
    "meta.fit(X_stack, y_all)\n",
    "final_oof = meta.predict(X_stack)\n",
    "print(f\">>> ACURÁCIA FINAL (Cross-Validation): {accuracy_score(y_all, final_oof):.5f}\")\n",
    "\n",
    "# Previsão Final\n",
    "final_probs = meta.predict_proba(X_stack_test)\n",
    "final_classes = np.argmax(final_probs, axis=1)\n",
    "\n",
    "# Mapeamento Reverso para Texto (Usando a ordem correta)\n",
    "# 0=None, 1=Low, 2=Medium, 3=High, 4=Very High\n",
    "reverse_map = {0: 'None', 1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}\n",
    "final_labels = [reverse_map[i] for i in final_classes]\n",
    "\n",
    "sub = pd.DataFrame({\"RowId\": np.arange(1, len(final_labels)+1), \"Result\": final_labels})\n",
    "sub.to_csv(\"submission_ultimate_v6.csv\", index=False)\n",
    "print(\"✅ Ficheiro 'submission_ultimate_v6.csv' gerado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d99ba8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ano                   int64\n",
       "Mes                   int64\n",
       "Dia                   int64\n",
       "Hora                float64\n",
       "Normal              float64\n",
       "HorarioEconomico    float64\n",
       "Autoconsumo         float64\n",
       "Injecao               int64\n",
       "temp                float64\n",
       "feels_like          float64\n",
       "temp_min            float64\n",
       "temp_max            float64\n",
       "pressure              int64\n",
       "humidity              int64\n",
       "wind_speed          float64\n",
       "rain_1h             float64\n",
       "clouds_all            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_treino.shape, df_teste.shape\n",
    "df_treino.head()\n",
    "df_treino.describe(include=\"all\")\n",
    "df_treino.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c18c5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Ler os dois arquivos CSV\n",
    "df_original = pd.read_csv(\"predicoes_injecao (2).csv\")\n",
    "df_substitutas = pd.read_csv(\"linhas_escolhidas.csv\")\n",
    "\n",
    "# 2. Usar \"RowId\" como chave de identificação\n",
    "chave = \"RowId\"\n",
    "\n",
    "df_original.set_index(chave, inplace=True)\n",
    "df_substitutas.set_index(chave, inplace=True)\n",
    "\n",
    "# 3. Substituir as linhas correspondentes\n",
    "df_original.update(df_substitutas)\n",
    "\n",
    "# 4. Salvar o novo dataset atualizado\n",
    "df_original.to_csv(\"predicoes_injecao_atualizado.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
